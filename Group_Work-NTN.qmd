---
date: last-modified
bibliography: bio.bib
csl: harvard-cite-them-right.csl
title: Next To Normal's Group Project
execute:
  echo: false
  freeze: true
format:
  html:
    code-copy: true
    code-link: true
    toc: true
    toc-title: On this page
    toc-depth: 2
    toc_float:
      collapsed: false
      smooth_scroll: true
  pdf:
    include-in-header:
      text: |
        \addtokomafont{disposition}{\rmfamily}
    mainfont: Spectral
    sansfont: "Roboto Flex"
    monofont: "Liberation Mono"
    papersize: a4
    geometry:
      - top=25mm
      - left=40mm
      - right=30mm
      - bottom=25mm
      - heightrounded
    toc: false
    number-sections: false
    colorlinks: true
    highlight-style: github
jupyter:
  jupytext:
    text_representation:
      extension: .qmd
      format_name: quarto
      format_version: '1.0'
      jupytext_version: 1.15.2
  kernelspec:
    display_name: Python 3 (ipykernel)
    language: python
    name: python3
---

## Declaration of Authorship {.unnumbered .unlisted}

We, [Next To Normal], pledge our honour that the work presented in this assessment is our own. Where information has been derived from other sources, we confirm that this has been indicated in the work. Where a Large Language Model such as ChatGPT has been used we confirm that we have made its contribution to the final submission clear.

Date:15/12

Student Numbers: 25109433,25243075,25063842,25223683,25245242

## Priorities for Feedback

Are there any areas on which you would appreciate more detailed feedback if we're able to offer it?

{{< pagebreak >}}

**Remove this page (up to the next `pagebreak`) prior to submission!**

# Code Examples

This page has example code to show you can include outputs while hiding code in Quarto, as well as some tools for interpolating data in the text. 

See the raw file for examples of how to hide computational output as there is code hidden here.

```{python}
#| echo: True 
# This code will be shown, 
# but the next block will not
import os
import pandas as pd
```

```{python}
# This code is not shown because echo was
# set to False in the header. You should
# think carefully about whether you want
# or need to show any code to the reader
# in the PDf.
host = 'https://orca.casa.ucl.ac.uk'
path = '~jreades/data'
file = '20250615-London-listings.parquet'

if os.path.exists(file):
  df = pd.read_parquet(file)
else: 
  df = pd.read_parquet(f'{host}/{path}/{file}')
  df.to_parquet(file)
```

An inline citation example: As discussed on @insideairbnb, there are many...

A parenthetical citation example: There are many ways to research Airbnb [see, for example, @insideairbnb]... 

```{python}
#| output: asis
# This is one way to write dynamic text. 'As is' means
# it will interpret the result as regular Quarto markdown 
# so the below `print` statement becomes plain text with
# the size of the dataframe interpolated into the string.
# When Quarto is done running the Python code what's left
# is plain-text and that is then treated as a paragraph
# like it would be if you'd written it in Quarto.
print(f"One of way to embed output in the text looks like this: after cleaning, we were left with {df.shape[0]:,} rows of data.")
```

```{python}
# The other way is to ensure that your variables are *simple* 
# data types that Quarto can understand. So notice below that
# we just have a single, inline `{python} with the value of
# the row directly inserted into the sentence. his should also work, but is sometimes less predictably
# reliable.
row_count = f"{df.shape[0]:,}"
```
The other way is to interpolate it directly into the sentence like this (`{python} row_count `); however, I've found it less reliable and it really requires you to be careful about data types. So `{python} print(f"{df.shape[0]:,}") ` probably won't work and you'll just see some Python code instead.

And here's a nice little chart straight into the document!

```{python}
ax = df.host_listings_count.plot.hist(bins=50);
ax.set_xlim([0,500]);
```

{{< pagebreak >}}

# Briefing
## Executive Summary

**Main takeaway**  
（1–2 句话，总结：Airbnb 在伦敦是否“失控”，以及是否需要监管）

**Key findings**  
- Airbnb 活动在伦敦呈现明显的空间不均衡分布，专业房东高度集中于……
- 少数专业房东控制了不成比例数量的房源，显示出明显的商业化特征。
- 若实施该提案，受影响的物业主要集中在……
- 该政策在缓解住房压力方面具有潜在益处，但也可能带来……
- 从社会流动性的角度看，该政策可被重新叙述为……
（上面是ai写的，我们需要结合自己的结论写摘要）：要注意：不写方法不写代码不写“我们将分析”完全模仿报告的摘要风格

## Background

简要概述伦敦短期租赁当前的争议焦点。
阐述反对党提出的政策建议及其引发争议的原因。
之前写的背景：#需要改
Background 近期，市长社会流动性顾问因非法出租公有暂住房屋的丑闻，将伦敦住房短缺与财富不平等的尖锐矛盾推向公众视野。 反对党借机提出强硬提案，要求对短期租赁（Airbnb）的房东实施强制登记并提高市政税，声称要平衡被 Airbnb 破坏的租赁和酒店市场。

本简报旨在评估这一政治驱动提案的经济合理性、社会公平性及对市长竞选的潜在政治影响。 现有分析表明，Airbnb 的商业化运营正在系统性地加剧伦敦的住房不平等问题。 造成这一现象的核心原因在于： 1.商业化挤占住房： 平台上职业房东和非法房源的存在，挤占了私人长租和酒店业的生存空间，从而损害了私人租房者的利益，并间接推高了区域房价。 2.加速区域财富不平等： 租金和房价上涨加剧了房主与非房主之间的财富差距。同时，房租压力使得低收入家庭向外伦敦迁移，加速了区域贫困的郊区化趋势。 3.社区旅游化： 商业运营引入了负面外部性（如噪音和滋扰），拉低了社区生活质量，导致社区走向“旅游化”。

## Analytical Framework and Data


这个所有人写完之后我来综合


# Question
## Q1: Is Airbnb “out of control” in London?

**Short answer**  
（一句话交代：例如
Airbnb activity in London is highly concentrated among a small subset of hosts, particularly professional hosts operating entire-home listings.）

**Evidence:**  
- 列数据，简要阐述数据，是什么，数据为（number)，数据表示了什么。


**Interpretation**  
详细阐述These patterns suggest that Airbnb activity in London is better understood
as spatially concentrated rather than uniformly excessive.

```{python}
# 这里才写 Python
import pandas as pd
import geopandas as gpd

df = pd.read_csv("data/listings.csv")
df.head()
```

## Q2: How many professional landlords are there?
London has a total of 5,222 professional hosts, accounting for 9.4% of all Airbnb hosts.
### Definition of Professional Host: 
Refers to individuals operating two or more entire-home listings. This threshold is commonly used in Airbnb literature to distinguish sustained commercial activity from temporary home-sharing (@sainaghi2021mom). Adopting this specific definition aims to more accurately reflect the phenomenon of entire residences being withdrawn from residential use, thereby establishing a more direct link between this group and long-term housing market pressures.
However, to reflect the heterogeneity within this group, professional landlords are further categorized by **spatial location (inner and outer London)** and **rental orientation (short-term and long-term)**. The inner versus outer London distinction is based on established research findings: London's housing pressures and wealth inequality exhibit a structural distribution along this boundary (@travers2016housing, p. 17), while dense commercial activity often concentrates in transit-oriented urban cores (@arcaute2016cities). Following this theoretical framework, the City of London is included within the inner core area.

### Distribution and Classification
- Professional hosts are unevenly distributed across space and rental orientation:
  - **Inner-city long-term professional hosts:** 3,590  
  - **Inner-city short-term professional hosts:** 108  
  - **Outer-city long-term professional hosts:** 921  
  - **Outer-city short-term professional hosts:** 603  

![Composition of Professional Hosts by Location and Rental Orientation](https://raw.githubusercontent.com/Taoo2025/casa0013_fsds_finalwork/fddef7c18e555c3704d4330345c058536fb0d83b/Composition%20of%20Professional%20Hosts%20by%20Location%20and%20Rental%20Orientation.png)
**Definitions and Analytical Scope**

The grouping results confirm that professional hosts are not a homogeneous group, showing marked variation in operating intensity and spatial distribution. While they remain a minority in numerical terms, their intensive use of housing resources makes them one of the most influential groups on the platform.
This suggests that debates over whether Airbnb is “out of control” should focus on how different types of professional hosts use housing, rather than solely relying on host counts. This finding provides essential context for evaluating regulatory proposals.

#放到后面数据分析Q4去，The KDE and MSOA-level analyses presented here further reveal a pronounced concentration of professional hosts in Inner London, whereas their presence in Outer London is comparatively sparse. This spatial asymmetry indicates that the disruptive effects of Airbnb are likely to be unevenly distributed across the city. Consequently, policy proposals that apply uniform registration and taxation measures without accounting for inner–outer differences risk both economic inefficiency and unintended social inequity.

#放到后面数据分析Q4去，KDE and MSOA-level analyses further reveal that such high-intensity short-term rentals are spatially concentrated in Inner London. This evidence suggests that Airbnb’s disruptive effects are unevenly distributed and underscores the need for differentiated policy responses rather than uniform regulation.

```{python}
# Q2
df = pd.read_csv("https://github.com/Taoo2025/casa0013_fsds_finalwork/raw/refs/heads/main/data/listings.csv") 

# Check the data content and format
print(df.head())
print(df.shape)

# Unify the column names
df.columns = df.columns.str.lower()
key_columns = [
    "id",
    "host_id",
    "room_type",
    "availability_365",
    "maximum_nights",
    "neighbourhood_cleansed",
    "latitude",
    "longitude"
]
# Back up data to safeguard against contamination of the original data.
df_select = df[key_columns].copy()
# numeric coercion
df_select["availability_365"] = pd.to_numeric(
    df_select["availability_365"],
    errors="coerce"
)

df_select["maximum_nights"] = pd.to_numeric(
    df_select["maximum_nights"],
    errors="coerce"
)
# Select the required columns
df_clean = (
    df_select
    .dropna(subset=[
        "availability_365",
        "maximum_nights",
        "host_id",
        "room_type",
        "neighbourhood_cleansed",
        "latitude",
        "longitude"
    ])
    .reset_index(drop=True)
)
# Inspect the dataframe before and after cleaning
print(df_clean.head())
print("Before NA cleaning:", len(df_select))
print("After NA cleaning:", len(df_clean))
# Filter properties
## 1.Only entire properties are available
df_entire = df_clean[df_clean["room_type"] == "Entire home/apt"].copy()
print(df_entire.head())
## 2.Count the number of entire properties owned by each landlord
host_entire_counts = (
    df_entire
    .groupby("host_id")
    .size()
    .reset_index(name="n_entire_homes")
)

## 3.Professional landlords: ≥2 properties
professional_hosts = host_entire_counts[
    host_entire_counts["n_entire_homes"] >= 2
]
## 4.Return to the listing level
df_professional = df_entire.merge(
    professional_hosts[["host_id"]],
    on="host_id",
    how="inner"
)
# Review the results of property listings screening
print(df_professional.head())
# Count the total number of landlords specialising in statistics.
n_professional_hosts = professional_hosts.shape[0]
print("Number of professional hosts:", n_professional_hosts)
# Review the affected properties
entire_counts_by_host = (
    df_professional
    .groupby("host_id")
    .size()
)

total_entire_listings_professional = entire_counts_by_host.sum()

print("Total entire-home listings owned by professional hosts:",
      total_entire_listings_professional)
# Take a look at the average number of entire properties owned by each professional landlord.
avg_entire_per_professional = entire_counts_by_host.mean()

avg_entire_per_professional
# Take a look at the median number of entire properties owned by professional landlords.

median_entire_per_professional = entire_counts_by_host.median()

median_entire_per_professional
# Inspect the current dataframe
print(len(df_professional))
print(df_professional.head())
# Defining the Inner and Outer Cities
## First, examine how the existing data names the London boroughs.
df_professional["neighbourhood_cleansed"].unique()
## Select the inner city area
## Inner London is defined as a set of central boroughs, including the City of London, which constitutes the historic and functional core of London and is consistently classified as part of Inner London in official statistics.
inner_boroughs = [
    "Camden",
    "Hackney",
    "Islington",
    "Kensington and Chelsea",
    "Lambeth",
    "Southwark",
    "Tower Hamlets",
    "Westminster",
    "Hammersmith and Fulham",
    "Wandsworth",
    "Lewisham",
    "Haringey",
    "Newham",
    "City of London"
]
## Use `np.where()` to create a new column named `inner_city`
df_professional["inner_city"] = np.where(
    df_professional["neighbourhood_cleansed"].isin(inner_boroughs),
    "Inner London",
    "Outer London"
)
#Define short-term and long-term rental categories, with the threshold set at 90 days.
##Further grouping based on short-term versus long-term rentals, though uncertain whether to select the 'availability_365' column or the 'maximum_nights' column. Therefore, first examine the median and mode of these two columns to observe the distribution pattern.
###The median availability of 189 days indicates that more than half of professional listings are unavailable for at least 176 days per year, suggesting a substantial degree of functional withdrawal from the housing market.
###By contrast, the median and modal maximum night setting of 365 indicates that most professional hosts impose no effective upper limit on booking duration, reflecting platform rule configuration rather than realised rental behaviour.
###Maximum night settings are used to identify the intended market role of professional hosts, distinguishing listings designed as long-term or quasi-hotel assets. Availability-based measures are subsequently employed to assess the intensity of commercial operation and its potential social externalities.
median_availability = df_professional["availability_365"].median()
mode_availability = df_professional["availability_365"].mode()
median_min_nights = df_professional["maximum_nights"].median()
mode_min_nights = df_professional["maximum_nights"].mode()
print("Availability_365:")
print("  Median:", median_availability)
print("  Mode(s):", mode_availability.tolist())

print("\nmaximum_nights:")
print("  Median:", median_min_nights)
print("  Mode(s):", mode_min_nights.tolist())
##Availability_365:
  #Median: 189.0
  #Mode(s): [0]

##maximum_nights:
 #Median: 365.0
  #Mode(s): [365]
# Define rental_role
df_professional["rental_role"] = np.where(
    df_professional["maximum_nights"] > 90,
    "Long-term oriented",
    "Short-term oriented"
)

#Calculate days_unavailable，Often regarded as proxy for usage intensity
df_professional["days_unavailable"] = (
    365 - df_professional["availability_365"]
)
#Quickly verify whether the new column has been successfully created and confirm that the data appears reasonable.
print(df_professional.head())

#Grouping
host_level = (
    df_professional
    .groupby("host_id")
    .agg(
        n_listings=("id", "count"),

        # Classifying landlords into inner and outer districts (majority principle)
        share_inner_city=("inner_city", lambda x: (x == "Inner London").mean()),

        # Classify landlords using short and long groups (majority principle, based on maximum_nights)
        share_long_term_role=("rental_role", lambda x: (x == "Long-term oriented").mean()),

        # Operational intensity (descriptive, not used for classification)
        median_days_unavailable=("days_unavailable", "median"),
        mean_days_unavailable=("days_unavailable", "mean")
    )
    .reset_index()
)

# Generate dominant_location, determining the landlord's primary spatial activity area based on the majority of property listings.
host_level["dominant_location"] = np.where(
    host_level["share_inner_city"] >= 0.5,
    "Inner London",
    "Outer London"
)

#Generate dominant_rental_role，share_long_term_role indicates the proportion of properties under this landlord's name that are systemically classified as 'long-term rental-oriented'.
host_level["dominant_rental_role"] = np.where(
    host_level["share_long_term_role"] >= 0.5,
    "Long-term oriented",
    "Short-term oriented"
)
#“This classification reflects the dominant pattern rather than the full heterogeneity of individual portfolios.”

#Create a new column host_group in host_level (where each row represents one professional host), and initially assign all hosts the value "Unclassified".
host_level["host_group"] = "Unclassified"
#Use loc with conditional statements to assign hosts into four categories.
host_level.loc[
    (host_level["dominant_location"] == "Inner London") &
    (host_level["dominant_rental_role"] == "Short-term oriented"),
    "host_group"
] = "Inner-city short-term professional hosts"

host_level.loc[
    (host_level["dominant_location"] == "Inner London") &
    (host_level["dominant_rental_role"] == "Long-term oriented"),
    "host_group"
] = "Inner-city long-term professional hosts"

host_level.loc[
    (host_level["dominant_location"] == "Outer London") &
    (host_level["dominant_rental_role"] == "Short-term oriented"),
    "host_group"
] = "Outer-city short-term professional hosts"

host_level.loc[
    (host_level["dominant_location"] == "Outer London") &
    (host_level["dominant_rental_role"] == "Long-term oriented"),
    "host_group"
] = "Outer-city long-term professional hosts"
#View grouping results
host_level["host_group"].value_counts()
#View the proxy metric for 'actual operational intensity'.
host_level.groupby("host_group")[[
    "median_days_unavailable",
    "mean_days_unavailable"
]].describe()

#visualisation
##Check the number of specialist landlords in each group
group_counts = (
    host_level["host_group"]
    .value_counts()
    .sort_index()
)
group_counts
##The proportion of the four categories among professional landlords
group_share = group_counts / group_counts.sum() * 100
import matplotlib.pyplot as plt
soft_blues = [
    "#4C72B0",
    "#55A868",
    "#C44E52",
    "#8172B3"
]


plt.figure(figsize=(6, 6))

plt.pie(
    group_share,
    labels=group_share.index,
    autopct="%.1f%%",
    startangle=90,
    colors=soft_blues,
    textprops={"fontsize": 10}
)

plt.title(
    "Composition of Professional Hosts by Location and Rental Orientation",
    fontsize=12
)

plt.axis("equal")
## Save image
### Define the save path
output_dir = "plotsQ2"  
filename = "Composition of Professional Hosts by Location and Rental Orientation.png"

### Create the folder (if it does not exist)
if not os.path.exists(output_dir):
    os.makedirs(output_dir)

### Stitch together the complete path
file_path = os.path.join(output_dir, filename)

### Save image
plt.savefig(file_path, dpi=300, bbox_inches="tight")
print(f"The image has been saved to:{file_path}")
plt.show()

##The proportion of professional landlords among all landlords

total_hosts = df_clean["host_id"].nunique()
professional_hosts_count = host_level["host_id"].nunique()

host_type_counts = pd.Series(
    {
        "Professional hosts": professional_hosts_count,
        "Non-professional hosts": total_hosts - professional_hosts_count
    }
)
host_type_counts
### Drawing
colors_simple = ["#4C72B0", "#DDDDDD"]

plt.figure(figsize=(5, 5))

plt.pie(
    host_type_counts,
    labels=host_type_counts.index,
    autopct="%.1f%%",
    startangle=90,
    colors=colors_simple,
    textprops={"fontsize": 10}
)

plt.title(
    "Share of Professional Hosts Among All Hosts",
    fontsize=12
)

plt.axis("equal")

### Define the save path
output_dir = "plotsQ2" 
filename = "Share of Professional Hosts Among All Hosts.png"

### Create the folder (if it does not exist)
if not os.path.exists(output_dir):
    os.makedirs(output_dir)

### Stitch together the complete path
file_path = os.path.join(output_dir, filename)

### Save image
plt.savefig(file_path, dpi=300, bbox_inches="tight")
print(f"Image saved to:{file_path}")
plt.show()
```
## Q3: How many properties would be affected by the proposal?

**Short answer:**
According to the analysis, 31755 entire home listings would be affected by the proposal.

**Evidence:**
The analysis filters the dataset to entire home listings and identifies professional hosts as those operating two or more such listings. All entire home listings operated by these hosts are counted as affected properties.

**Interpretation:**
The number of affected properties is produced through a sequence of explicit filtering and aggregation steps. The analysis begins by restricting the dataset to entire home listings only. This step reflects the assumption embedded in the proposal that entire home short term rentals are the relevant unit of impact. Listings that are private rooms or shared rooms are excluded at this stage and do not contribute to the final count.

The code then classifies hosts according to the number of entire home listings they operate. Hosts with two or more entire home listings are labelled as professional hosts. This classification is applied uniformly across the dataset and does not rely on external information such as ownership records or income data. Once hosts are classified in this way the affected properties are defined mechanically as all entire home listings associated with professional hosts. The final number of affected properties is therefore not inferred or estimated but counted directly from the listings that meet these criteria.

An important implication of this approach is that the affected count reflects listings rather than hosts. A single professional host may contribute multiple affected properties to the total. This explains why the number of affected properties is large relative to the number of hosts identified as professional. The analysis treats each entire home listing as a separate property that would fall under the proposal regardless of whether listings belong to the same host. As a result, the count captures the scale of housing units potentially influenced by the proposal rather than the number of individuals subject to regulation.

The analysis also uses unavailable nights to describe the activity associated with these listings but this measure does not change which properties are counted as affected. Instead, it provides additional context on how intensively the affected listings are used. The affected property count itself depends only on listing type and host classification and remains stable regardless of variation in activity levels across listings.

Overall, the results show that the proposal would apply to a clearly defined and countable set of properties derived from transparent rules implemented in the code. The figure of 31755 affected properties is therefore best interpreted as a direct description of scope within the dataset rather than as a prediction of behavioural change or housing outcomes.

```{python}
import pandas as pd
import numpy as np
from typing import Dict

import geopandas as gpd
from sklearn.preprocessing import MinMaxScaler

import matplotlib.pyplot as plt
import seaborn as sns
import matplotlib.patches as mpatches

from pathlib import Path
Path("output").mkdir(exist_ok=True)

# =========================
# 0) CONFIG
# =========================
AIRBNB_CSV_PATH   = r"listings.csv"
MSOA_GPKG_PATH    = r"QGIS_wk1_gepkg.gpkg"
MSOA_LAYER        = "qgis_wk1_gepkg__london_borough_fly_tipping"
OUTPUT_PNG_PATH   = "output/airbnb_kde_borough_groups.png"

WEIGHTS: Dict[str, float] = {
    "I1_professional": 0.30,
    "I2_entire_home": 0.35,
    "I3_potential_violation": 0.15,
    "I4_economic_potential": 0.20
}

# =========================
# 1) Borough group definitions (based on The literature 'Travers, Tony, Sam Sims and Nicholas Bosetti. 2016. "Housing and Inequality in London". Centre for London. https://www.centreforlondon.org/wp-content/uploads/2016/08/CFLJ4292-London-Inequality-04_16_WEB_V4.pdf', defines the inner city boundaries (Travers et al. 2016:17).
# =========================
INNER_WEST = [
    "Camden",
    "City of London",
    "Westminster", 
    "Kensington and Chelsea",
    "Hammersmith and Fulham",
    "Wandsworth",
]

INNER_EAST = [
    "Haringey",
    "Hackney",
    "Islington",
    "Tower Hamlets",
    "Newham",
    "Lambeth",
    "Southwark",
    "Lewisham",
]

OUTER_EAST_AND_NORTH_EAST = [
    "Enfield",
    "Waltham Forest",
    "Redbridge",
    "Barking and Dagenham",
    "Havering",
    "Greenwich",
    "Bexley",
]

OUTER_WEST_AND_NORTH_WEST = [
    "Barnet",
    "Brent",
    "Harrow",
    "Hillingdon",
    "Ealing",
    "Hounslow",
    "Richmond upon Thames",
]

OUTER_SOUTH = [
    "Kingston upon Thames",
    "Merton",
    "Sutton",
    "Croydon",
    "Bromley",
]


def make_borough_group_map() -> dict:
    group_map = {}
    for n in INNER_WEST:
        group_map[n] = "Inner West"
    for n in INNER_EAST:
        group_map[n] = "Inner East"
    for n in OUTER_EAST_AND_NORTH_EAST:
        group_map[n] = "Outer East & North East"
    for n in OUTER_WEST_AND_NORTH_WEST:
        group_map[n] = "Outer West & North West"
    for n in OUTER_SOUTH:
        group_map[n] = "Outer South"
    return group_map


def add_borough_groups_strict(boundary_gdf: gpd.GeoDataFrame, name_col: str = "NAME") -> gpd.GeoDataFrame:
    """
    Add 'group' column. No 'Unclassified':
    - If any borough cannot be mapped, raise an error and print missing names.
    """
    gdf = boundary_gdf.copy()
    gdf[name_col] = gdf[name_col].astype(str).str.strip()

    # normalize (handle & vs and)
    gdf["_name_norm"] = (
        gdf[name_col]
        .str.replace("&", "and", regex=False)
        .str.replace("  ", " ", regex=False)
        .str.strip()
    )

    group_map = make_borough_group_map()
    group_map_norm = {k.replace("&", "and").strip(): v for k, v in group_map.items()}

    gdf["group"] = gdf["_name_norm"].map(group_map_norm)
    missing = gdf.loc[gdf["group"].isna(), name_col].tolist()

    gdf = gdf.drop(columns=["_name_norm"])

    if missing:
        raise ValueError(
            "Some boroughs were not assigned to a group (check spelling in your lists):\n"
            + "\n".join(missing)
        )

    # fixed legend order
    from pandas.api.types import CategoricalDtype
    order = CategoricalDtype(
        categories=[
            "Inner West",
            "Inner East",
            "Outer East & North East",
            "Outer West & North West",
            "Outer South",
        ],
        ordered=True
    )
    gdf["group"] = gdf["group"].astype(order)
    return gdf


# =========================
# 2) LOAD boundary from GPKG
# =========================
def load_msoa_from_gpkg(gpkg_path: str, layer: str | None) -> gpd.GeoDataFrame:
    if layer is not None:
        gdf = gpd.read_file(gpkg_path, layer=layer)
    else:
        gdf = gpd.read_file(gpkg_path)

    if not isinstance(gdf, gpd.GeoDataFrame) or "geometry" not in gdf.columns:
        raise TypeError(f"Layer '{layer}' is not a spatial layer (no geometry). Choose a polygon layer.")
    if gdf.geometry.isna().all():
        raise ValueError(f"Layer '{layer}' geometry is empty (all NA). Choose a different layer.")

    if gdf.crs is None:
        gdf = gdf.set_crs("EPSG:27700")

    return gdf.to_crs("EPSG:4326")


# =========================
# 3) AIRBNB LOAD + COMPOSITE
# =========================
def load_and_prepare_data(csv_path: str) -> pd.DataFrame:
    df = pd.read_csv(csv_path)

    if "price" in df.columns:
        df["price"] = (
            df["price"].astype(str)
            .str.replace("$", "", regex=False)
            .str.replace(",", "", regex=False)
        )
        df["price"] = pd.to_numeric(df["price"], errors="coerce")
    else:
        df["price"] = np.nan

    for col in ["room_type", "host_listings_count", "maximum_nights", "availability_365",
                "estimated_revenue_l365d", "estimated_occupancy_l365d"]:
        if col not in df.columns:
            df[col] = np.nan

    df["room_type"] = df["room_type"].fillna("Unknown")
    df["host_listings_count"] = pd.to_numeric(df["host_listings_count"], errors="coerce").fillna(0)

    df = df.dropna(subset=["latitude", "longitude"]).copy()
    return df


def calculate_composite_index_point(df: pd.DataFrame, weights: Dict[str, float]) -> pd.DataFrame:
    df["I1_professional"] = (df["host_listings_count"] >= 3).astype(int)
    df["I2_entire_home"] = (df["room_type"] == "Entire home/apt").astype(int)

    df["maximum_nights"] = pd.to_numeric(df["maximum_nights"], errors="coerce").fillna(0)
    df["availability_365"] = pd.to_numeric(df["availability_365"], errors="coerce").fillna(0)
    df["I3_potential_violation"] = (
        (df["maximum_nights"] > 90) &
        (df["availability_365"] >= 180) &
        (df["I2_entire_home"] == 1)
    ).astype(int)

    df["estimated_revenue_l365d"] = pd.to_numeric(df["estimated_revenue_l365d"], errors="coerce")
    df["estimated_occupancy_l365d"] = pd.to_numeric(df["estimated_occupancy_l365d"], errors="coerce").fillna(0)
    df["price"] = pd.to_numeric(df["price"], errors="coerce").fillna(0)

    df["I4_economic_potential"] = df["estimated_revenue_l365d"].fillna(
        df["price"] * df["estimated_occupancy_l365d"] * 365
    )

    cols = list(weights.keys())
    df[cols] = df[cols].fillna(0)

    scaler = MinMaxScaler()
    df[cols] = scaler.fit_transform(df[cols])

    df["D_composite_index"] = 0.0
    for c, w in weights.items():
        df["D_composite_index"] += df[c] * w

    return df


# =========================
# 4) PLOT (borough-group zoning)
# =========================
def plot_kde_with_context(df_w: pd.DataFrame, boroughs: gpd.GeoDataFrame, output_png: str, label_names: bool = True):
    df_w = df_w[df_w["D_composite_index"] > 0].dropna(subset=["longitude", "latitude", "D_composite_index"])
    if df_w.empty:
        raise ValueError("No valid weighted points for KDE plotting.")

    # add groups (strict: no Unclassified)
    boroughs = add_borough_groups_strict(boroughs, name_col="NAME")

    # outline for bbox + border
    try:
        outline_geom = boroughs.to_crs("EPSG:27700").geometry.union_all()
    except Exception:
        outline_geom = boroughs.to_crs("EPSG:27700").geometry.unary_union
    outline = gpd.GeoSeries([outline_geom], crs="EPSG:27700").to_crs("EPSG:4326")

    fig, ax = plt.subplots(1, 1, figsize=(14, 14))

    group_colors = {
        "Inner West": "#B71346",
        "Inner East": "#ED6D85",
        "Outer East & North East": "#75C7EB",
        "Outer West & North West": "#D8EEFA",
        "Outer South": "#ABDAF2",
    }

    # 1) group background fill
    boroughs.plot(
        ax=ax,
        color=boroughs["group"].map(group_colors),
        alpha=0.85,
        edgecolor="none"
    )

    # 2) group boundaries dashed grey
    boroughs.boundary.plot(
        ax=ax,
        color="white",
        linewidth=1.2,
       
        alpha=0.9
    )

    # 3) KDE
    sns.kdeplot(
        x=df_w["longitude"],
        y=df_w["latitude"],
        weights=df_w["D_composite_index"],
        cmap="Oranges",
        fill=True,
        alpha=0.55,
        levels=15,
        ax=ax
    )

    # 4) borough boundaries thin (optional)
    boroughs.plot(ax=ax, facecolor="none", edgecolor="white", linewidth=0.8, alpha=0.9)

    # 5) bold outline
    outline.plot(ax=ax, facecolor="none", edgecolor="black", linewidth=1.8, alpha=1.0)

    # 6) Borough name labels (optional)
    if label_names:
        label_gdf = boroughs.copy()
        label_gdf["label_point"] = label_gdf.geometry.representative_point()
        for _, row in label_gdf.iterrows():
            ax.text(
                row["label_point"].x,
                row["label_point"].y,
                row["NAME"],
                fontsize=7,
                ha="center",
                va="center",
                color="black",
                alpha=0.75
            )

    # legend (ordered)
    legend_order = [
        "Inner West",
        "Inner East",
        "Outer East & North East",
        "Outer West & North West",
        "Outer South",
    ]
    handles = [mpatches.Patch(color=group_colors[k], alpha=0.25, label=k) for k in legend_order]
    ax.legend(handles=handles, loc="lower left", frameon=True)

    # extent
    minx, miny, maxx, maxy = outline.total_bounds
    ax.set_xlim(minx, maxx)
    ax.set_ylim(miny, maxy)

    ax.set_title("Airbnb weighted KDE with borough-based zones", fontsize=14)
    ax.set_xlabel("Longitude")
    ax.set_ylabel("Latitude")

    plt.tight_layout()
    plt.savefig(output_png, dpi=300)
    plt.show()
    print(f"Saved: {output_png}")


# =========================
# 5) MAIN
# =========================
def main():
    print("--- Loading borough boundaries from GeoPackage ---")
    boundary = load_msoa_from_gpkg(MSOA_GPKG_PATH, MSOA_LAYER)

    print("--- Loading Airbnb data ---")
    df = load_and_prepare_data(AIRBNB_CSV_PATH)

    print("--- Building composite index ---")
    df_w = calculate_composite_index_point(df, WEIGHTS)

    print("--- Plotting ---")
    plot_kde_with_context(df_w, boundary, OUTPUT_PNG_PATH, label_names=True)

    print("--- Done ---")

if __name__ == "__main__":
    main()
    
# =========================
# 6) BOROUGH LEVEL ANALYSIS ( Practical-07 & Practical-09)
# =========================
def analyze_borough_impact(
    gpkg_path: str,
    layer: str,
    csv_path: str,
    weights: dict
):
    print("\n--- 6. Start the analysis of the impact of administrative region levels ---)

    # 1. Loading boundary
    boundary_wgs84 = load_msoa_from_gpkg(gpkg_path, layer)

    # 2. Load and calculate the composite index
    df = load_and_prepare_data(csv_path)
    df_w = calculate_composite_index_point(df, weights)

    df_w = df_w.dropna(subset=["longitude", "latitude", "D_composite_index"])
    if df_w.empty:
        print("❌  Error: There is no valid weighted point data, so spatial connection cannot be performed. ")
        return None

    # 3. Point → GeoDataFrame
    points_gdf = gpd.GeoDataFrame(
        df_w,
        geometry=gpd.points_from_xy(df_w.longitude, df_w.latitude),
        crs="EPSG:4326"
    )

    # 4. Spatial connection
    boundary_bng = boundary_wgs84.to_crs("EPSG:27700")
    points_bng = points_gdf.to_crs("EPSG:27700")

    joined_gdf = gpd.sjoin(
        points_bng,
        boundary_bng[["NAME", "geometry"]],
        how="inner",
        predicate="intersects"
    )

    # 5. Administrative District Aggregation (⚠️ Must be within the function)
    borough_impact = (
        joined_gdf
        .groupby("NAME")
        .agg(
            mean_composite=("D_composite_index", "mean"),
            listing_count=("id", "count")
        )
        .sort_values(by="mean_composite", ascending=False)
    )

    print("\n--- Ranking of Average Impact Index of Administrative Regions (Top 10) ---")
    print("Measuring the average intensity of commercialization and high risks in each administrative region.") 
    print(borough_impact.head(10))

    return borough_impact



# =========================
# 7) PLOT BAR CHART (combine Practical-10)
# =========================
def plot_borough_impact_bar(borough_impact: pd.DataFrame, top_n: int = 10):
    """Draw a bar chart showing the top-ranking administrative regions in terms of influence."""
    if borough_impact is None or borough_impact.empty:
        print("❌ NO rawing result: Administrative region impact data is empty.)
        return

    top_impact = borough_impact.head(top_n)

    plt.figure(figsize=(10, 6))

    sns.barplot(
        x="mean_composite",
        y=top_impact.index,
        data=top_impact,
        palette="Reds_d"
    )

    plt.title(
        f"Top {top_n} London Boroughs by Mean Composite Airbnb Impact Index",
        fontsize=14
    )
    plt.xlabel("Mean Composite Index ($D_{composite_index}$)")
    plt.ylabel("Administrative Area (Borough/MSOA)")

    plt.tight_layout()
    plt.savefig("data_use/borough_impact_bar_chart.png", dpi=300)
    plt.show()

    print("Saved: data_use/borough_impact_bar_chart.png")



# =========================
# 8) UPDATED MAIN FUNCTION
# =========================
def main_with_analysis():
    # Run the original KDE drawing process
    main() 

    # Run the analysis of the newly added administrative region levels
    borough_results = analyze_borough_impact(
        MSOA_GPKG_PATH,
        MSOA_LAYER,
        AIRBNB_CSV_PATH,
        WEIGHTS
    )

    # Draw a bar chart of the results
    plot_borough_impact_bar(borough_results, top_n=10)

    # Summary of Results
    if borough_results is not None:
        top_borough = borough_results.iloc[0]
        print(f"\n--- Final summary ---")
        print(f"The administrative region with the highest average composite index is：{borough_results.index[0]}")
        print(f"The average index of this area：{top_borough['mean_composite']:.4f}")
        print(f"The total number of housing units in this area：{top_borough['listing_count']} ")
        
# Run the new main function
if __name__ == "__main__":
    main_with_analysis()

```

## Q4: What are the likely pros and cons of the proposal?

**Data analysis (Evidence)**
This assessment draws on three strands of empirical evidence:
- scale and proportion indicators identifying who would be affected;
- host-level operating intensity measured through unavailable nights (KDE);
- MSOA-level spatial distributions highlighting uneven policy impacts.

In terms of scale, professional hosts account for only 9.4% of all Airbnb hosts in London, yet they control 32.9% of all listings, 50.8% of entire-home listings, and 41.4% of total operating nights. This indicates a high regulatory leverage: targeting a relatively small group has the potential to affect a large share of housing use intensity (see @fig-combined-metrics and @fig-policy_matrix).


![combined_metrics](https://raw.githubusercontent.com/Taoo2025/casa0013_fsds_finalwork/6a28bdfe9ca0bc13f277ebe0b1a7b3b2801aa72f/combined_metrics.png)


![Policy matrix](https://raw.githubusercontent.com/Taoo2025/casa0013_fsds_finalwork/6a28bdfe9ca0bc13f277ebe0b1a7b3b2801aa72f/policy_matrix.png){#fig-policy_matrix fig-cap="Policy matrix" width=80%}



Operating intensity is highly uneven across professional hosts. KDE results show that inner-city short-term professional hosts exhibit the highest intensity, with a median of approximately 293 unavailable nights per year, reflecting strongly commercialised use. In contrast, outer-city long-term professional hosts show much lower intensity (median ≈ 142 nights), closer to residential or semi-commercial use.


MSOA-level maps further reveal a spatial asymmetry. High-intensity short-term activity is concentrated in a small number of central MSOAs, while outer-city long-term activity is spatially dispersed with relatively low local pressure. This suggests that policy benefits and policy burdens are unlikely to fall in the same places.

Operating intensity is highly uneven across professional hosts. As shown in @fig-distribution-operating-intensity-inner-city,KDE results show that inner-city short-term professional hosts exhibit the highest intensity, with a median of approximately 293 unavailable nights per year, reflecting strongly commercialised use. In contrast, outer-city long-term professional hosts show much lower intensity (median ≈ 142 nights), closer to residential or semi-commercial use.

![Distribution of Operating Intensity – Inner-city Short-term Professional Hosts](https://raw.githubusercontent.com/Taoo2025/casa0013_fsds_finalwork/48779e52ebb83ed31a2676ca9df67d76614840bc/distribution_operating_intensity_inner_city.png)

As shown in @fig-uneven-operating-intensity-leverage-vs-costs，MSOA-level maps further reveal a spatial asymmetry. High-intensity short-term activity is concentrated in a small number of central MSOAs, while outer-city long-term activity is spatially dispersed with relatively low local pressure(see @fig-msoa-operating-nights-outer-city-long-term). This suggests that policy benefits and policy burdens are unlikely to fall in the same places. As shown in @fig-msoa-concentration-operating-nights-inner-city-short-term, MSOA-level analysis indicates that operating nights among inner-city short-term professional hosts are highly concentrated in a small number of central MSOAs.


![MSOA-level Operating Nights Outer-city Long-term Professional Hosts](https://raw.githubusercontent.com/Taoo2025/casa0013_fsds_finalwork/48779e52ebb83ed31a2676ca9df67d76614840bc/MSOA-level%20Operating%20Nights%20Outer-city%20Long-term%20Professional%20Hosts.png)

![Uneven Spatial Distribution of Operating Intensity: High Leverage vs Uneven Costs](https://raw.githubusercontent.com/Taoo2025/casa0013_fsds_finalwork/48779e52ebb83ed31a2676ca9df67d76614840bc/Uneven%20Spatial%20Distribution%20of%20Operating%20Intensity%20High%20Leverage%20vs%20Uneven%20Costs.png)
![MSOA-level Concentration of Operating Nights Inner-city Short-term Professional Hosts](https://raw.githubusercontent.com/Taoo2025/casa0013_fsds_finalwork/48779e52ebb83ed31a2676ca9df67d76614840bc/MSOA-level%20Concentration%20of%20Operating%20Nights%20Inner-city%20Short-term%20Professional%20Hosts.png)




### Implications for the Mayor

**Potential benefits**  
- The proposal offers high regulatory leverage. By targeting around 9.4% of Airbnb hosts, it addresses over 40% of entire-home operating nights, allowing the Mayor to demonstrate decisive action with a relatively limited regulatory scope. The policy narrative is also clear: data consistently point to inner-city short-term professional hosts as the main drivers of commercialised housing use, closely aligning with public concerns about Airbnb being “out of control”.

**Potential drawbacks**  
- The policy risks being perceived as overly broad. A substantial share of affected professional hosts are outer-city, long-term oriented landlords whose contribution to housing pressure is comparatively modest. As long-term oriented hosts account for roughly 86% of affected professional landlords, the proposal may provoke political resistance from groups not central to the core problem, potentially weakening electoral support and policy legitimacy.

### Implications for Residents
**Potential benefits**  
- Residents living in the most affected inner-city neighbourhoods are likely to benefit the most, as the regulation focuses on areas where homes are most frequently taken out of residential use and short-term tourist activity is most intense. The spatial evidence reflects residents’ everyday experiences of noise, overcrowding, and neighbourhood disruption.

**Potential drawbacks**  
- The benefits are not evenly shared. Residents in outer London may experience little direct improvement in housing conditions, while still being indirectly affected by the policy. This creates concerns about fairness, as communities that face fewer Airbnb-related pressures may still bear part of the policy’s social and economic costs. 

### Implications for the City
**Potential benefits**  
- From a city-wide perspective, the proposal targets a spatially concentrated problem. KDE and MSOA analyses show that the most intensive short-term use of entire homes is clustered in a limited number of inner-city areas, making enforcement and monitoring more feasible. The policy also aligns strategically with broader urban goals by directly addressing touristification and housing pressure in central neighbourhoods.

**Potential drawbacks**  
- The spatial distribution of costs and benefits is uneven. While policy gains are generated mainly in high-pressure inner-city areas, a significant share of regulatory burden falls on outer London, where professional hosting activity is more dispersed and less intensive. Without spatial or intensity-based differentiation, the policy risks inefficient resource allocation and may impose disproportionate costs on areas that are not the primary source of the problem.


```{python}
#Below is a cost-benefit analysis and the data analysis it requires.
## A. Basic Preparation: Measuring Days Unavailable
## B. Baseline Scale of the Entire Airbnb Market (Denominator)
## C. Entire-Home Listings as the Core Source of Housing Pressure
## D. Policy-Relevant Target Group: Professional Hosts
## E. Coverage Ratios: Scale-Based Evidence of Policy Reach
## F. Host-Level Structure of Four Types of Professional Hosts
## G. Combined Short-Term / Long-Term Rental Perspective
## H. Three-Sided Cost–Benefit Matrix
# Basic Preparation: Measuring Days Unavailable
import pandas as pd
import numpy as np

# Ensure that there is days_unavailable
if "days_unavailable" not in df_clean.columns:
    df_clean["days_unavailable"] = 365 - df_clean["availability_365"]
    
#  Baseline Scale of the Entire Airbnb Market (Denominator)
total_listings = df_clean["id"].nunique()
total_hosts = df_clean["host_id"].nunique()

# Entire-Home Listings as the Core Source of Housing Pressure
df_entire = df_clean[df_clean["room_type"] == "Entire home/apt"].copy()

total_entire_listings = df_entire["id"].nunique()
total_entire_nights = df_entire["days_unavailable"].sum()

#Entire-home listings ≈ dwellings that are potentially “functionally withdrawn” from residential use.
#Total operating nights ≈ the intensity with which housing resources are consumed by the market;
#Policy-Relevant Target Group: Professional Hosts
affected_listings = df_professional["id"].nunique()
affected_hosts = df_professional["host_id"].nunique()
affected_nights = df_professional["days_unavailable"].sum()
#Coverage ratio indicator (core evidence of policy merits)
share_affected_hosts = affected_hosts / total_hosts
share_affected_listings_all = affected_listings / total_listings
share_affected_listings_entire = affected_listings / total_entire_listings
share_affected_nights_entire = affected_nights / total_entire_nights
# At the listing level, actual operating nights are aggregated by host.
host_nights = (
    df_professional
    .groupby("host_id")
    .agg(
        sum_days_unavailable=("days_unavailable", "sum")
    )
    .reset_index()
)
print(host_level.columns.tolist())
host_level["days_unavailable_intensity"] = host_level["median_days_unavailable"]
host_level.columns
#F.Host-level: Actual Operating Structure of Four Types of Professional Hosts
#F1.First, ensure that the host level contains “actual total nights”
# The host_level dataset should already include:
# - n_listings
# - days_unavailable_intensity

host_group_summary = (
    host_level
    .groupby("host_group")
    .agg(
        n_hosts=("host_id", "count"),
        avg_n_listings=("n_listings", "mean"),
        total_nights=("days_unavailable_intensity", "sum"),
        avg_nights=("days_unavailable_intensity", "mean")
    )
    .reset_index()
)
#F2.The proportion within the professional landlord sector
host_group_summary["share_of_prof_hosts"] = (
    host_group_summary["n_hosts"] / host_group_summary["n_hosts"].sum()
)

host_group_summary["share_of_prof_nights"] = (
    host_group_summary["total_nights"] / host_group_summary["total_nights"].sum()
)
#G. Combined Short-Term / Long-Term Rental Perspective
host_group_summary["term_type"] = np.where(
    host_group_summary["host_group"].str.contains("long-term", case=False),
    "Long-term oriented",
    "Short-term oriented"
)

term_summary = (
    host_group_summary
    .groupby("term_type")
    .agg(
        n_hosts=("n_hosts", "sum"),
        total_nights=("total_nights", "sum")
    )
    .reset_index()
)

term_summary["share_of_prof_hosts"] = (
    term_summary["n_hosts"] / term_summary["n_hosts"].sum()
)
term_summary["share_of_prof_nights"] = (
    term_summary["total_nights"] / term_summary["total_nights"].sum()
)

#H. Stakeholder perspective (for an electoral briefing)
#H1. Mayor
benefit_mayor = pd.DataFrame({
    "metric": [
        "affected_listings_share_of_all",
        "affected_listings_share_of_entire",
        "affected_nights_share_of_entire"
    ],
    "value": [
        share_affected_listings_all,
        share_affected_listings_entire,
        share_affected_nights_entire
    ]
})

cost_mayor = pd.DataFrame({
    "metric": [
        "affected_hosts_share_of_all_hosts",
        "long_term_hosts_share_within_affected"
    ],
    "value": [
        share_affected_hosts,
        float(
            term_summary
            .loc[term_summary["term_type"] == "Long-term oriented",
                 "share_of_prof_hosts"]
            .iloc[0]
        )
    ]
})
#H2. City
city_benefit = pd.DataFrame({
    "metric": [
        "share_affected_listings_of_entire_homes",
        "share_affected_nights_of_entire_homes",
        "inner_short_nights_share_of_prof_nights"
    ],
    "value": [
        share_affected_listings_entire,
        share_affected_nights_entire,
        float(
            host_group_summary
            .loc[
                host_group_summary["host_group"]
                == "Inner-city short-term professional hosts",
                "share_of_prof_nights"
            ]
            .fillna(0)
            .iloc[0]
        )
    ]
})

city_cost = pd.DataFrame({
    "metric": [
        "outer_long_hosts_share_of_prof_hosts",
        "outer_long_nights_share_of_prof_nights"
    ],
    "value": [
        float(
            host_group_summary
            .loc[
                host_group_summary["host_group"]
                == "Outer-city long-term professional hosts",
                "share_of_prof_hosts"
            ]
            .fillna(0)
            .iloc[0]
        ),
        float(
            host_group_summary
            .loc[
                host_group_summary["host_group"]
                == "Outer-city long-term professional hosts",
                "share_of_prof_nights"
            ]
            .fillna(0)
            .iloc[0]
        )
    ]
})

#H3. Residents
resident_benefit = pd.DataFrame({
    "metric": [
        "affected_entire_listings_share",
        "affected_entire_nights_share"
    ],
    "value": [
        share_affected_listings_entire,
        share_affected_nights_entire
    ]
})

resident_cost = pd.DataFrame({
    "metric": [
        "affected_prof_hosts_share_of_all_hosts",
        "outer_long_term_prof_hosts_share"
    ],
    "value": [
        share_affected_hosts,
        city_cost.loc[
            city_cost["metric"] == "outer_long_hosts_share_of_prof_hosts",
            "value"
        ].iloc[0]
    ]
})

#Synthesised into a single “policy trade-off matrix”
def tag(df, stakeholder, side):
    out = df.copy()
    out["stakeholder"] = stakeholder
    out["side"] = side
    return out

combined_metrics = pd.concat([
    tag(benefit_mayor,   "Mayor",     "Benefit"),
    tag(cost_mayor,     "Mayor",     "Cost"),
    tag(city_benefit,   "City",      "Benefit"),
    tag(city_cost,      "City",      "Cost"),
    tag(resident_benefit, "Residents", "Benefit"),
    tag(resident_cost,    "Residents", "Cost"),
], ignore_index=True)
print(combined_metrics)

# First, define a helper function to aggregate indicators for each stakeholder.

def build_policy_matrix(metrics_df):
    # Create an empty matrix table
    stakeholders = metrics_df['stakeholder'].unique()
    sides = metrics_df['side'].unique()
    table = pd.DataFrame(index=sides, columns=stakeholders, dtype=object)
    
    # Complete the form
    for stakeholder in stakeholders:
        for side in sides:
            metric_map = metrics_df[(metrics_df['stakeholder'] == stakeholder) & (metrics_df['side'] == side)]
            cell_lines = []
            for _, row in metric_map.iterrows():
                label = row['metric']
                value = row['value']
                line = f"{label}: {value:.1%}"  # Convert the results into percentages
                cell_lines.append(line)
            table.loc[side, stakeholder] = "\n".join(cell_lines)
    return table

# Use the newly generated combined_metrics to construct the matrix.
policy_matrix = build_policy_matrix(combined_metrics)

# Display Matrix Table
import pandas as pd
print(policy_matrix)

import seaborn as sns
import matplotlib.pyplot as plt

#Filter rows from the host_level dataset where the group is “Inner-city short-term professional hosts”
#This subset represents the group of “inner-city short-term professional hosts”
inner_short_hosts = host_level[
    host_level["host_group"] == "Inner-city short-term professional hosts"
]
plt.figure(figsize=(9, 6))
#Use seaborn’s `kdeplot` to draw a kernel density estimation (KDE) plot.
sns.kdeplot(
    data=inner_short_hosts,
    x="mean_days_unavailable",
    fill=True,
    alpha=0.5
)

plt.title(
    "Distribution of Operating Intensity\nInner-city Short-term Professional Hosts",
    fontsize=13
)
plt.xlabel("Average Number of Unavailable Nights per Host")
plt.ylabel("Density")
# Define the save path
output_dir = "plotsQ2"  
filename = "Distribution of Operating Intensity\nInner-city Short-term Professional Hosts.png"

# Create folder (if it does not exist)
if not os.path.exists(output_dir):
    os.makedirs(output_dir)

# Stitch together the complete path
file_path = os.path.join(output_dir, filename)

# Save image
plt.savefig(file_path, dpi=300, bbox_inches="tight")
print(f"Image saved to:{file_path}")
plt.show()
# Retrieve London MSOA data
msoa = gpd.read_file("https://github.com/Taoo2025/casa0013_fsds_finalwork/raw/refs/heads/main/data_use/london_msoa_boundaries.gpkg") 
print(msoa.columns)
# Create a GeoDataFrame from a regular DataFrame

# A GeoDataFrame is the core data structure of the GeoPandas library, extending a standard DataFrame with geographic geometry information

gdf_listings = gpd.GeoDataFrame(
    df_professional,
    geometry=gpd.points_from_xy(
        df_professional["longitude"],
        df_professional["latitude"]
    ),
    crs="EPSG:4326"
)
gdf_listings = gdf_listings.to_crs(msoa.crs)

#Link listings to MSOAs (spatial linkage).
gdf_listings_msoa = gpd.sjoin(
    gdf_listings,
    msoa[[ "MSOA11CD", "geometry" ]],
    how="left",
    predicate="within"
)
#Extract `host_group` from `host_level`.
host_groups = host_level[["host_id", "host_group"]].copy()
#Merge it back to the listing level.
gdf_listings_msoa = gdf_listings_msoa.merge(
    host_groups,
    on="host_id",
    how="left"
)
print(gdf_listings_msoa.columns)
#Inner City · Short-Term Rentals · Professional Landlords
inner_short = gdf_listings_msoa[
    gdf_listings_msoa["host_group"]
    == "Inner-city short-term professional hosts"
]
msoa_inner_short = (
    inner_short
    .groupby("MSOA11CD")
    .agg(
        inner_short_nights=("days_unavailable", "sum"),
        inner_short_listings=("id", "count")
    )
    .reset_index()
)
# Left-join the MSOA geospatial data with the msoa_inner_short summary statistics using the MSOA11CD column
# MSOA11CD is the unique identifier for UK MSOAs (Middle Layer Super Output Areas) and serves as the join key
msoa = msoa.merge(
    msoa_inner_short,
    on="MSOA11CD",
    how="left"
)
# Handle missing values after the merge: fill NaN values in the inner_short_nights and inner_short_listings columns with 0
# Select these two columns, apply fillna(0), and then assign the result back to the original DataFrame
msoa[["inner_short_nights", "inner_short_listings"]] = (
    msoa[["inner_short_nights", "inner_short_listings"]]
    .fillna(0)
)
#Mapping the Core Policies of the MSOA
fig, ax = plt.subplots(1, 1, figsize=(8, 8))

msoa.plot(
    column="inner_short_nights",
    cmap="Reds",
    linewidth=0.2,
    edgecolor="grey",
    legend=True,
    scheme="NaturalBreaks",
    k=5,
    ax=ax
)

ax.set_title(
    "MSOA-level Concentration of Operating Nights\n"
    "Inner-city Short-term Professional Hosts",
    fontsize=12
)

ax.axis("off")

# Define the save path
output_dir = "plotsQ2"  
filename = "MSOA-level Concentration of Operating Nights Inner-city Short-term Professional Hosts.png"

# Create the folder (if it does not exist)
if not os.path.exists(output_dir):
    os.makedirs(output_dir)

# Stitch together the complete path
file_path = os.path.join(output_dir, filename)

# Save image
plt.savefig(file_path, dpi=300, bbox_inches="tight")
print(f"Image saved to:{file_path}")
plt.show()

# Filter listings from the GeoDataFrame gdf_listings_msoa for a specific group
# Selection criterion: host_group equals “Outer-city long-term professional hosts”
outer_long = gdf_listings_msoa[
    gdf_listings_msoa["host_group"]
    == "Outer-city long-term professional hosts"
]
# Group the data by MSOA11CD (the unique identifier for UK MSOA areas) and aggregate the statistics
# The result is the total number of unavailable nights attributable to outer-city long-term professional hosts for each MSOA
msoa_outer_long = (
    outer_long
    .groupby("MSOA11CD")
    .agg(
        outer_long_nights=("days_unavailable", "sum")
    )
    .reset_index()
)
msoa = msoa.merge(
    msoa_outer_long,
    on="MSOA11CD",
    how="left"
)

msoa["outer_long_nights"] = msoa["outer_long_nights"].fillna(0)

fig, ax = plt.subplots(1, 1, figsize=(8, 8))

msoa.plot(
    column="outer_long_nights",
    cmap="Blues",
    linewidth=0.2,
    edgecolor="grey",
    legend=True,
    scheme="NaturalBreaks",
    k=5,
    ax=ax
)

ax.set_title(
    "MSOA-level Operating Nights\n"
    "Outer-city Long-term Professional Hosts",
    fontsize=12
)

ax.axis("off")

# Define the save path
output_dir = "plotsQ2" 
filename = "MSOA-level Operating Nights Outer-city Long-term Professional Hosts.png"

# Create the folder (if it does not exist)
if not os.path.exists(output_dir):
    os.makedirs(output_dir)

# Stitch together the complete path
file_path = os.path.join(output_dir, filename)

# Save image
plt.savefig(file_path, dpi=300, bbox_inches="tight")
print(f"Image saved to:{file_path}")
plt.show()


# Compute the maximum value across the two indicators to serve as a shared upper bound (vmax) for both maps

# This ensures that colour intensity corresponds to the same value range in both figures, facilitating direct comparison

vmax = max(
    msoa["inner_short_nights"].max(),
    msoa["outer_long_nights"].max()
)

fig, axes = plt.subplots(1, 2, figsize=(14, 6))

# Benefit: inner-city short-term rentals
msoa.plot(
    column="inner_short_nights",
    cmap="Reds",
    linewidth=0.2,
    edgecolor="grey",
    legend=True,
    vmin=0,
    vmax=vmax,
    ax=axes[0]
)

#Set the title of the first subplot, using `\n` to insert a line break for improved layout and readability.
axes[0].set_title("Inner-city Short-term Professional Hosts\nOperating Nights")
axes[0].axis("off")

# Cost: outer-city long-term rentals
msoa.plot(
    column="outer_long_nights",
    cmap="Blues",
    linewidth=0.2,
    edgecolor="grey",
    legend=True,
    vmin=0,
    vmax=vmax,
    ax=axes[1]
)
#Set the title of the second subplot.
axes[1].set_title("Outer-city Long-term Professional Hosts\nOperating Nights")
axes[1].axis("off")
#Set the overall title of the figure.
plt.suptitle(
    "Uneven Spatial Distribution of Operating Intensity\n"
    "High Leverage vs Uneven Costs",
    fontsize=14
)
# Define the save path
output_dir = "plotsQ2"  
filename = "Uneven Spatial Distribution of Operating Intensity High Leverage vs Uneven Costs.png"

# Create the folder (if it does not exist)
if not os.path.exists(output_dir):
    os.makedirs(output_dir)

# Stitch together the complete path
file_path = os.path.join(output_dir, filename)

# Save image
plt.savefig(file_path, dpi=300, bbox_inches="tight")
print(f"Image saved to:{file_path}")
plt.show()


```
## Q5: Can the story be reframed positively in terms of social mobility?

**Short answer:**  
Yes, Airbnb’s uneven and seemingly uncontrolled growth can be reframed as a mechanism that makes social inequality visible, turning gentrification into a spatially governable issue and enabling a more equitable and strategic redistribution of economic opportunities and social resources.

**Evidence and framing:**  

- 1 Airbnb, a Contested Sharing Economy Platform, to a Governable Policy Object
Airbnb is one of the most prominent platforms within the so-called “sharing economy,” using peer-to-peer digital technology to connect hosts and guests and, in doing so, creating a form of short-term rental that sits between traditional residential letting and hotel accommodation (@wachsmuth2018airbnb). While short-term rental platforms are often framed in public debate as drivers of housing shortages, inequality, and gentrification, the findings of this study—read alongside the existing literature—suggest a more nuanced interpretation. Rather than viewing Airbnb solely as a source of housing pressure, it can also be understood as a lens through which underlying market dynamics become visible, creating opportunities for more targeted and effective urban governance.


- 2 Evidence of Spatial Heterogeneity
The evidence clearly shows that Airbnb’s impact on London’s housing market is not evenly distributed across the city. Although professional hosts make up only around 9% of all Airbnb hosts, they control approximately 51% of entire-home listings and account for about 41% of all entire-home booking nights. These activities are heavily concentrated in inner-city areas and are characterised by high-intensity, entire-home use, with a median operating intensity of around 293 nights per year. By contrast, professional hosts in outer-city areas operate at much lower intensity, with a median of approximately 142 nights per year. Their activity more closely resembles low-intensity, intermittent, or income-supplementary letting, rather than the systematic removal of housing from the long-term residential market. Mapping the spatial concentration of Airbnb listings further confirms that housing commercialisation pressures are clustered in a small number of inner-city areas, rather than spread evenly across London as a whole.


- 3 From “Gentrification” in London to the “Visualisation of Inequality”
In the London context, these spatial differences need to be understood against the city’s longer-term patterns of gentrification.
London has long displayed a clear inner–outer city divide: inner-city areas concentrate high-value employment, cultural capital, and investment-led housing demand, while outer-city areas primarily serve residential and social reproduction functions. This structural imbalance creates conditions in which gentrification pressures are more likely to emerge and become spatially concentrated (@gyodi2024spatial). Without effective guidance, short-term rentals in high-demand inner-city areas can reinforce these existing dynamics, accelerating gentrification, further undermining housing affordability, and increasing the risk of community displacement.

However, the spatial analysis in this study suggests that Airbnb does not shape this process in the same way across the city.
Rather than operating uniformly, it brings the sharp differences in housing use between inner and outer London into clearer view. Mapping the spatial concentration of listings shows that housing commercialisation pressures are tightly clustered in inner-city neighbourhoods, while outer-city areas display a distinctly different pattern characterised by low-intensity, supplementary short-term renting. From this perspective, short-term rentals are not simply a “gentrification accelerator,” but also a means through which previously less visible spatial inequalities become more apparent.

This pattern of spatial heterogeneity is consistent with findings in the existing literature. Previous research shows that Airbnb introduces a new and spatially uneven stream of potential rental income into housing markets, creating a “short-term rental rent gap” that is most pronounced in high-demand inner-city areas (@wachsmuth2018airbnb). At the same time, market-focused studies indicate that, under low-intensity conditions, short-term renting can serve as a way of making use of underutilised housing assets and strengthening household economic resilience (@barron2021effect). 

**Interpretation** 
Policy Implications and Recommendations

From a policy perspective, Airbnb’s expansion can be understood not simply as market activity, but as a process that reshapes housing use and highlights where pressures in the housing system are most concentrated.
In high-demand inner-city areas, short-term rentals quickly expose latent rent gaps, providing policymakers with clear and geographically identifiable targets for intervention. By contrast, in outer-city areas, legal and low-intensity short-term rental activity may offer households valuable income flexibility. Small-scale short-term rental income can help younger households manage mortgage payments or provide a supplementary source of income during periods of labour market instability, thereby strengthening household resilience. By providing an additional financial buffer, these arrangements can help individuals and families strengthen their economic resilience over time, supporting opportunities for upward social mobility across the life course.

On the other hand, equally important, making inequality visible has clear implications for how cities are governed. Rather than treating gentrification as a diffuse and hard-to-pin-down urban trend, this approach allows policymakers to identify risk areas based on concrete evidence.
As a result, the policy debate can move away from the abstract question of whether Airbnb should be restricted in general, and instead focus on more practical concerns: which locations and which forms of short-term rental activity pose genuine housing risks, and how targeted interventions can be designed without creating disproportionate social costs.

For the mayor, the key question is therefore not whether Airbnb should be tolerated or banned, but how different forms of short-term rental activity vary by location and operating intensity.
Uniform regulation, or “one-size-fits-all” restrictions, risk imposing disproportionate governance costs in non-core areas while weakening policy effectiveness in inner-city neighbourhoods where housing commercialisation pressures are most acute. By contrast, a guided approach to governance (being sensitive to spatial variation and differences in operating intensity) can both relieve housing pressures in inner-city areas and preserve the potential positive role of short-term rentals in supporting household resilience and social mobility.
Reframing Airbnb and its spatial distribution as a process that reveals, rather than simply produces, inequality can therefore support more transparent, targeted, and spatially responsive policymaking. This perspective does not deny the reality or risks of inner-city gentrification. Rather, it emphasises that by making these risks visible, measurable, and geographically specific, data-driven spatial analysis creates political and policy space for the mayor to pursue rational, differentiated action in an otherwise highly contentious housing debate.

Although our analysis shows that professional hosting activity can be clearly identified and spatially targeted using detailed listing data, the effectiveness of any regulatory intervention ultimately depends on whether it can be enforced in practice. Without reliable access to platform-level information, local authorities may struggle to identify which hosts meet regulatory thresholds or to monitor compliance over time.

This challenge has been observed in other regulatory contexts. Evidence summarised by the National Bureau of Economic Research suggests that short-term rental regulation becomes more effective once authorities can access systematic data from platforms. In cities such as Chicago, reductions in active short-term rental listings became more pronounced only after data-sharing arrangements were introduced between platforms and regulators (@nber2024str).

Similar concerns have been raised in the UK context. A House of Commons Library briefing on the growth of short-term lettings in England notes that enforcement of existing regulations, including the ninety-night limit in London, is constrained by limited access to accurate and comprehensive information on listing activity (@ukparl2024str).

In relation to our findings, improved data transparency would directly support the spatial targeting identified through MSOA analysis and host typologies. If boroughs can access information on listing locations, host identities, and operating intensity, enforcement resources could be directed more effectively toward inner-city areas where high-intensity professional activity is concentrated. This would increase the likelihood that regulation achieves its intended housing outcomes and would help ensure that enforcement is perceived as evidence-based and proportionate. 



## Overall Assessment

## Transparency and Limitations
- This analysis relies on scraped platform data and operational definitions of professional landlords, which may under- or over-estimate true levels of commercial activity. Spatial patterns should therefore be interpreted as indicative rather than definitive.
- Operating nights are proxied using availability data.
- The 90-day threshold is policy-relevant but imperfect.
- Results describe relative patterns rather than causal effects.
-The number of affected properties is calculated directly from the listing data using explicit filtering and counting steps implemented in the code. Professional hosts are defined based


{{< include _questions.qmd >}}

## References









